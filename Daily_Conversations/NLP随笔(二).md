当 AI 在某一个单点任务上的表现接近或者超越人类的时候，就会给行业带来巨大的商机。在视觉分类、检索、匹配、目标检测等各项任务上，随着相关算法越来越准确，业界也开始在大量商业场景中尝试这些技术

深度学习在计算机视觉、语音识别等感知智能技术上率先取得成功并不是偶然。深度学习秉承连接主义学派的范式，相较传统统计机器学习技术的最大进化在于其利用了高于统计方法数个数量级的参数和极其复杂的函数组合，通过引入各种非线性和多层级感知能力，构成了远强于统计机器学习模型的拟合能力。ResNet-152 的参数量已经达到六千万的级别，GPT-2.0 的参数量达到了惊人的 15 亿。而其他上亿甚至数亿级别的网络更是数不胜数。如此复杂的模型对数据的拟合能力达到了前所未有的水平，但是同时也极大提高了过拟合的风险。这对数据提出了极高的要求。训练数据的数量、维度、采样均衡度、单条数据本身的稠密度（非0、不稀疏的程度），都需要达到极高的水平，才能将过拟合现象降低到可控范围。

视觉信息（图像、视频）恰好是这样一类自然连续信号：一张图片通常就有数百万甚至上千万像素，而且每个像素上通常都有颜色，数据量大、数据的表示稠密、冗余度也高。往往在丢失大量直接视觉信号的情况下，人还能迅速理解图片的语义信息，就是因为自然连续信号，如图像中的场景和物体往往具有视觉、结构和语义上的共性。一个 30MB 的位图图片能被压缩到 2MB 而让人眼基本无法感知区别；一个 30MB 的 wave 音频文件被压缩到 3MB 的 MP3 还能基本保持主要旋律和听感，都是因为这类自然连续信号中存在大量不易被人的感官所感知的冗余。

视觉信息这种的丰富和冗余度，让深度神经网络得以从监督信号中一层层提炼、一层层感知，最终学会部分判断逻辑。深度神经网络在感知智能阶段中在视觉任务和语音任务上的成功，离不开视觉、语音信号自身的这种数据特点

![](https://maoxianxin1996.oss-accelerate.aliyuncs.com/codechina1/20210803152422.png)

今天，属于感知智能的视觉和语音应用已经全面开花，但属于认知智能的自然语言处理却发展滞后。这种发展状态与自然语言处理技术中的数据特征也有密不可分的关系。 

相对于图片、语音给出的直接信号，文字是一种高阶抽象离散信号。较之图片中的一个像素，文本中一个单元信息密度更大、冗余度更低，往往组成句子的每一个单词、加上单词出现的顺序，才能正确表达出完整的意思。如何利用单个文本元素（字/词）的意思，以及如何利用语句中的顺序信息，是近年来自然语言处理和文本分析技术的主要探索脉络

![](https://maoxianxin1996.oss-accelerate.aliyuncs.com/codechina1/20210803152442.png)

2013 年，词的分布式向量表示（Distributed Representation）出现之前，如何在计算机中高效表示单个字/词是难以逾越的第一个坎。在只能用One-hot向量来表示字/词的年代，两个近义词的表示之间的关系却完全独立，语义相似度无法计算；上表示一个字/词所需的上万维向量中只有一个维度为1，其他维度都为0，稀疏度极高。面对这类信号，深度神经网络这类复杂的模型所擅长的化繁为简的抽象、提炼、总结能力便束手无策，因为输入信号已经极简到了连最基础的自我表示都难以做到。

而分布式词向量将语言的特征表示向前推进了一大步。分布式词向量提出了一个合理的假设：两个词的相似度，可以由他们在多个句子中各自的上下文的相似度去度量，而上下文相似的两个词会在向量空间中由两个接近的向量来表示。这种做法部分赋予了词向量“语义”，因此我们不必再让机器去查百科全书告诉我们“苹果”的近义词是“梨子”，而是直接从大量的互联网语料中去学习，原来“苹果”的近义词也可以是“三星”、“华为”。因为人们常常会说“我购买了一个苹果手机”，也常说“我购买了一个三星手机”，模型会敏锐的学习到“苹果”和“三星”在大量语料中出现时其上下文高度相似，因而认为两个词相似。分布式词向量让无语义、极稀疏的 One-hot 向量寿终正寝，而为大家提供了嵌入语义信息、稠密的特征表示，这才使得深度神经网络在自然语言处理和文本分析上的应用真正变得可能。

捕捉语句中在独立的词集合基础之上、词序列构成的句子结构信息也是自然语言处理和文本分析中的一个主要方向。传统条件随机场（CRF）考虑了前后相邻元素和当前元素之间的依赖；长短时记忆网络模型（LSTM）以一种衰减形式考虑了当前元素之前的元素序列；seq2seq 通过注意力和编解码的机制使得解码时的当前元素不光能用上已经解码完毕的元素序列，还能用上编码前的序列的完整信息；近期各类基于 Transformer 结构，如 ELMo 、BERT、GPT-2.0、XLNet，则利用两阶段（基于自编码的预训练加基于任务的调优）模式，能够以自监督的方式更好地利用大规模的无标注语料训练不同句子结构中词语之间的关系，并且突破传统线性序列结构中存在的难以建立长距离、双向依赖关系的问题，学习到质量更高的中间语言模型，再通过调优就能在文本生成、阅读理解、文本分类、信息检索、序列标注等多个任务上取得当前最为领先的准确率。

为自然语言任务加入“常识”，也是另一个新兴重要探索方向，这个方向则与知识图谱技术紧密结合

就像 BERT、GPT-2.0、XLNet 在两阶段范式上的殊途同归，我们也认为基础语言模型在不同任务上可以存在一些不变性，但在不同场景中一定要做特殊语料与任务下的调优与适配

但认知智能在金融、公安、媒体等场景中的变化部分给 AI 厂商带来的挑战非常明显。一个算法往往在不同场景下要利用不同的标注语料去形成不同的模型，一个媒体场景的 10 类新闻分类模型，无法给另一个媒体的 12 类分类体系使用